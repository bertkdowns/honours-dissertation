\chapter{Data Preprocessing} \label{sec:datapreprocessing}

% It's required because process engineers get raw sensor data and need to aggregate it etc for the model.
% However, they also need to work from historical data to test the model, and to understand the system.

As concluded in \cref{sec:history}, it is required that a comprehensive Digital Twin Platform support the workflows of process engineers, who are responsible to link the raw sensor data to the model. This workflow is seperate from the workflow to design a model, and thus requires a seperate set of tools. 
In this chapter, a simple data preprocessing pipeline is added to the Ahuora Platform to support this workflow. 

When developing a way to link a simulation to real-time data, raw historical data is generally used to test the data processing pipeline, because it allows to test against a standardised set of conditions. Then, the same pipeline is used to process the real-time data when the solution is deployed.
Thus, the tools to process historical data should be the same as the tools to process live data. Building these tools in the Ahuora Digital Twin platform will provide a foundation to evaluate how to implement more advanced data processing techniques.

\section{Minimum Viable Pipeline}

% todo: add an example schema/drawing of what it should look like
The simplest implementation that fits these requirements is a way to set properties to many different values, from some external data source (e.g a CSV file, or a live data stream). Thus the simulation could be run with many different conditions. Each successive simulation would be added to the history, so all simulations could be compared.
This requires a way to load in data, and a way to specify which properties should be updated. It needs to be done in a way that can be easily extended from the simple ``CSV file'' case to a live data stream.

As with the history feature added in \cref{sec:history}, this feature also is a core requirement of the Ahuora Simulation platform. It is required to support multi-steady-state simulation, a common type of analysis for chemical engineers. Thus it is an appropriate place to start development. 

% This is also part of multi-steady-state and pretty helpful anyway for chemical engineers too.
\section{Design}
The tool that chemical engineers would be most familiar with for this task is a spreadsheet. Thus, it was decided to design the platform in a similar structure. The user would need to upload a csv file, and all the data could be viewed in a table. As in a spreadsheet, additional "columns" could be added to the table, with calculated values based on the row.

This paradigm makes sense from a chemical engineer's standpoint, and from a live data processing standpoint. Each "row" can represent one point in time, with all the data avaliable at that point in time. The "columns" can represent different properties, and can be linked to properties in the simulation. In a live data processing system, due to performance reasons you often are only able to access the current row, and perhaps aggregation functions or other simple calculations from previous rows. This can be easily replicated in this "spreadsheet" view, as we can limit the user to only being able to access the current row, and the previous row.

% todo: add a picture of a spreadsheet wtih some data to explain this better
\section{Implementation}

The most complex part of this process is to implement formula processing. Formulas need to be able to reference only certain other properties, and need to be able to be parsed from strings. SymPy was chosen as the library to do this, as it is a well-known library for symbolic mathematics in Python, so it supports the Ahuora Backend, and has support for complex enough expressions for this use case. It also provides a layer of security over using Python's built-in eval function, as it only evaluates mathematical expressions.
Sympy's parsing libraries support specifying a dictionary of variables that can be used in parsing. This can be used to pass the contents of previously calculated columns, and columns of raw data, to the parser. 

To keep the implementation simple, the CSV file was stored in the browser's local storage, and a seperate request was made to the server to solve the flowsheet at each timestep. The solve request would include the values from the CSV, for that timestep. Though it may not be the most efficient method for batch processing, this most closely mirrors the way that a live data processing system works. 

The frontend was modified to allow the user to upload a CSV file, and to view the data in a table. The user can add new columns to the table, and specify a formula for the column. A column could be linked to a property in the simulation via a new "link" button on the flowsheet page. 
% TODO: Add images of the ui at thsoe different stages.

Results of the simulation can be viewed through the flowsheet, and through the history page.The user interface workflow between these pages could be improved, but is not the focus of this project: the main concern is functionality and architecture.

\section{Evaluation}

The implementation of the data preprocessing pipeline provides valuable intuition into the next steps to link the Ahuora Simulation Platform with live data processing systems. The preprocessing functionality provides a method to parameterise the simulation, abstracting away the details of the internal setup of the simulation. The previous methods involved making multiple API calls to set each property, and was more heavily dependent on the exact implementation. This method allows Process Engineers to have more control over how live data is processed, without needing to rely on the way that a Data Engineer would link it up with the live data source, because it allows them to decide what they will expose to the live data processing system. It also enables different models to be constructed from the same data source, which could be helpful for debugging and testing, or modelling different operation modes.


% explain the result, how it worked

% How did it ifluence the architecture of the system?




% IT was implemented using XYZ, because XYZ so that ABC
% also, we decided that ... because ... so that ...



